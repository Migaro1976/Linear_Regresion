# Implementing a Linear Regression problem with Python 

In linear regression, our hypothesis function â„ğœƒ is:

  â„ğœƒ(ğ‘¥)=ğœƒ0+ğœƒ1ğ‘¥

And, as we are doing regression, our cost function is: ğ½(ğœƒ0,ğœƒ1)=1ğ‘šâˆ‘ğ‘–=1ğ‘š(ğ‘¦Ì‚ ğ‘–âˆ’ğ‘¦ğ‘–)2=1ğ‘šâˆ‘ğ‘–=1ğ‘š(â„ğœƒ(ğ‘¥ğ‘–)âˆ’ğ‘¦ğ‘–)2
Nota that, the cost funtion is just the sum of all the square errors from our hypothesis (ğ‘¦Ì‚ ğ‘–) versus the data (ğ‘¦ğ‘–).

The best parameters for our hypothesis will give us the minimum cost function.

Finding a minimum for J
Finding a minimum of a function is equivalent to finding the parameters that make the gradient of that function to vanish. In other words:

âˆ‡ğœƒğ½(ğœƒ)=0
We will implement two ways of solving this problem.

# A) Gradient descent (Numerical method)
From a starting point (ğœƒ), we will try to move to a new point ğœƒâ€², decreasing the cost funtion (ğ½(ğœƒ)). We will do this many times, up to we find a minimum (or close enough to it).

Partial differentials of the cost function (using chain rule)
âˆ‚ğ½âˆ‚ğœƒ0=2ğ‘šâˆ‘ğ‘–=1ğ‘š(â„ğœƒ(ğ‘¥ğ‘–)âˆ’ğ‘¦ğ‘–)
âˆ‚ğ½âˆ‚ğœƒ1=2ğ‘šâˆ‘ğ‘–=1ğ‘š(â„ğœƒ(ğ‘¥ğ‘–)âˆ’ğ‘¦ğ‘–)â‹…ğ‘¥ğ‘–
Finally, we need to update iteratively the values for ğœƒ0 and ğœƒ1. Using Gradient Descent algorithm with learning rate (ğ›¼) until convergence criterion (ğœ–) is achieved:

     while (convergence==False):
ğœƒâ€²0=ğœƒ0âˆ’ğ›¼âˆ‚ğ½âˆ‚ğœƒ0
ğœƒâ€²1=ğœƒ1âˆ’ğ›¼âˆ‚ğ½âˆ‚ğœƒ1
ğ½â€²=ğ½(ğœƒâ€²0,ğœƒâ€²1)
Î”ğ½=ğ‘ğ‘ğ‘ (ğ½â€²âˆ’ğ½)
ğ‘ğ‘œğ‘›ğ‘£ğ‘’ğ‘Ÿğ‘”ğ‘’ğ‘›ğ‘ğ‘’=(Î”ğ½<ğœ–)

# B) Normal equations (Algebra)
In matrix notation, we can implement our hypothesis as:

â„ğœƒ(ğ‘¥(ğ‘–))=(ğ‘¥(ğ‘–))ğ‘‡ğœƒ
Note that, in this case, if we want to consider our hypothesis such â„(ğœƒ)=ğœƒ0+ğœƒğ‘–ğ‘¥(ğ‘–) where ğ‘¥ is a vector, for consistency, we need to introduce an additional "constant feature" in our data. In other words, we need to map our input data as follows:

ğ‘¥ğ‘–â†’[1,ğ‘¥ğ‘–]
we can express gradient of J as follows:

âˆ‡ğœƒğ½(ğœƒ)=ğ‘‹ğ‘‡ğ‘‹ğœƒâˆ’ğ‘‹ğ‘‡ğ‘¦âƒ— 
To minimize J, we set its derivatives to zero, therefore obtaining the normal equations:

ğ‘‹ğ‘‡ğ‘‹ğœƒ=ğ‘‹ğ‘‡ğ‘¦âƒ— 
We can solve this equation for theta.

As a final remark, we can extend this method to non linear hypothesis by extending our input data ğ‘¥ to the features we need. For example, for a parabolic fit:

ğ‘¥ğ‘–â†’[1,ğ‘¥ğ‘–,ğ‘¥2ğ‘–]

# Problem
Giving the data provided below (x->y), find the best equation fit, using:
-Gradient Descent
-Normal Equations
Using your own python implementation, using numpy and scipy tools (not scipy!).
